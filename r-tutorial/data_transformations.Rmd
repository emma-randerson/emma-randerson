---
title: "R Tutorial"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Welcome to R

There are two main types of script we tend to use in R at Peak, R scripts and R Markdown files. 

* Scripts are what you'll usually use to write code day-to-day in order to (eventually) deploy it into production
* R Markdown is useful when you'd like to create a document, such as a HTML file or PDF with your work in - we often use Markdown files for insight reports

I'm using R Markdown today so that you can easily read this document after I've taken you through it, it's also an easy way for me to show you the code to run to generate certain results.

## A whistle stop tour of R studio

You may or may not have used R Studio before, so here's a quick rundown of where the most useful things are:

* **Console**: The R console should show up underneath the script you have open. The console shows the code that is being executed. Press cmd+Enter on a line of code in your script and it will run in the console.
* **Environment**: This is where objects are stored e.g. if you read in data, and call it `transactions`, an object called `transactions` will appear in the environment pane which you can interact with. If it's a data frame object, you can click on it and a viewing panel will be displayed so you can have a look at your data in a nice format!
* **Files**: Here you can see which files you have saved in your Workspace. If it's your first time using a Workspace, this will be empty, so you will need to clone repositories from Github that contain the code you'd like to work on.
* **Plots**: When you create plots, they will display here. There's a useful 'Export' button that can be used to export your charts in the right dimensions if you'd like to include them in a presentation.
* **Packages**: This allows you to see which R packages are installed & allows you to install packages if you need one that is not currently on the Workspace. You can also use the `install.packages` function directly to do this e.g. `install.packages("hello")` would install the hello package (if that exists?!)
* **Terminal**: This is like a normal Terminal, similar to what you will find on your laptop and also in the Jupyter Workspaces. Here you can run git commands in order to make sure you're tracking and saving changes with your work.

## Loading libraries

At the beginning of your script, you should load the libraries you will be using within your code. This makes it clear what the dependencies are and which packages you need to have installed for the code to run correctly.

This tutorial will focus on the following R packages:

* `dplyr`: Data transformations and aggregations - this makes it great for exploring your data
* `dbplyr`: Uses the same function names as `dplyr`, but converts it to SQL so that you can perform aggregations over a database, rather than reading all of the raw data into R
* `ggplot2`: Makes charts - it uses layers to build up highly customisable charts
* `odbc`: Allows us to connect to Redshift data bases and run SQL queries on them
* `stringr`: Can do useful things with strings, such as take substrings, replace certain strings etc.

```{r }
# Load in libraries required for the script
library(dplyr); library(dbplyr); library(ggplot2); library(odbc); library(stringr)
```

We will also be using a few packages that have been developed internally at Peak. You will need to install these using the `devtools` package.

* `S3R`: Reads in data from S3. Our internal alternative to the `aws.s3` package.
* `peak-theme`: Makes charts generated by `ggplot2` Peak branded (e.g. Peak colours, fonts)

You'll need a personal access token from Github in order to do this.

```{r, eval = FALSE}
# Install peaktheme
devtools::install_github('PeakBI/peak-theme',
                         auth_token = '<insert Github personal access token here>')

# Install S3R
devtools::install_github('PeakBI/S3R',
                         auth_token = '<insert Github personal access token here>')
```

```{r }
library(peaktheme); library(S3R);
```

Sometimes you might not know all of the packages you want to use in your script - that's okay, just make sure to add them to the top of your script as you go.

## Loading data

At Peak we store data in both S3 and Redshift. Here are some examples of how to load data from both places!

### S3

The `s3.read_using` function in the `S3R` package can be used to read in a CSV file saved in S3 and call it `transactions`. The `FUN` argument can be changed depending on what type of file you'd like to read in; in this example it's using the `read.csv` function that is in base R, but it could also use `read_csv` in the readr package if you prefer that!

The `glimpse` function in `dplyr` can be used as a quick way to preview your data. From that, we can see that our data contains 2,595,732 rows and 12 columns.

```{r }
# Read in transactional data from S3
transactions <- s3.read_using(FUN = read.csv,
                              bucket = 'kilimanjaro-prod-datalake',
                              object_path = 'newstarter/uploads/transaction_data/1607445741950_Peak_transaction_data.csv')

# Preview the data
glimpse(transactions)
```

### Redshift

Now let's replicate the same, but assuming that our data is stored in Redshift instead... Before you can read in the data, you will need to setup a connection to the database your data is stored in. After that, you can run a SQL query to read in the data you'd like to use.

```{r }
# Connect to Redshift (this code can be found in the Connections tab under New Connection)
con <- dbConnect(odbc::odbc(), "newstarter-prod", bigint = "integer")

# Read in data using a SQL query
transactions <- dbGetQuery(con, "select * from dunnhumby.transactions")

glimpse(transactions)
```

You can also run entire SQL scripts in R if you have a complex query you'd like to run.

```{r }
# Add in read file SQL stuff here
```

## Data transformation using dplyr

`dplyr` is an R package that provides you with functions that will help you to clean and transform your data. It is part of the `tidyverse` set of packages which are considered the go-tos for anyone using R on a regular basis.

Here are some useful functions in the `dplyr` package:

* `select` defines which columns of your data frame you'd like to keep
* `filter` removes any unwanted rows from your data frame
* `mutate` adds new columns to your data frame or modify existing columns
* `left_join`, `full_join`, `inner_join` etc. can be used to join two tables by a specified key

Below are a few examples of how to use `dplyr` on a data set.

### select

For this exercise, we aren't looking at anything to do with coupons, so you may decide to remove it from your data altogether.

```{r }
# Option 1 (list the columns you want to keep)
transactions_select_1 <- transactions %>%
  select(household_key, basket_id, day, product_id, quantity,
         sales_value, store_id, retail_disc, trans_time, week_no)

glimpse(transactions_select_1)

# Option 2 (list the columns you want to remove, with a - sign in front of each one)
transactions_select_2 <- transactions %>%
  select(-coupon_disc, -coupon_match_disc)

glimpse(transactions_select_2)
```

### filter

You might want to remove rows that aren't relevant for the analysis you are doing, or you may want to look at certain rows in more detail.

Let's assume we want to look at the most recent week's worth of data in more detail. The maximum week in the data set is 102, so you can do this using the following.

```{r }
# Example 1 - filtering a single column on a single value
transactions_filter_1 <- transactions %>%
  filter(week_no == 102)

glimpse(transactions_filter_1)
```

You might want to include week 101 too, which you can do using `%in%` instead of `==`:

```{r }
# Example 2 - filtering a single column on multiple values
transactions_filter_2 <- transactions %>%
  filter(week_no %in% c(101,102))

glimpse(transactions_filter_2)
```

You can also filter on multiple columns at the same time, which is the equivalent of `AND` logic. For example, if we wanted the data for weeks 101 and 102 for store 400, you could do the following:

```{r }
# Example 3 - filtering on multiple columns
transactions_filter_3 <- transactions %>%
  filter(week_no %in% c(101,102), store_id == 400)

glimpse(transactions_filter_3)
```

### mutate

The `mutate` function allows you to add new columns to your data, or change existing columns (e.g. if the column needs cleaning).

In the data, `trans_time` is stored as an integer, when it is in fact a time. To be on the safe side and make sure you don't misuse this column, you may wish to convert it to a string for now.

```{r }
# Convert trans_time column to character
transactions_mutate_1 <- transactions %>%
  mutate(trans_time = as.character(trans_time))

glimpse(transactions_mutate_1)
```

We may want to do things like extract the hour of this time. There are probably lots of ways you can do this! In this example, the last 2 digits of the `trans_time` column are removed.

Aside: the `end = -3` argument uses negative indices i.e. the last digit in the string is labelled 1, second to last 2 etc. As we want to remove the final two digits from the substring, we want to create a substring between the first digit and the third last digit - this is where the -3 comes from.

```{r }
# Add an hour column based on trans_time
transactions_mutate_2 <- transactions %>%
  mutate(trans_hour = as.numeric(str_sub(trans_time, end = -3)))

glimpse(transactions_mutate_2)
```

You can do multiple mutates at the same time! So the code below will achieve the same as the two steps above, but in one step!

```{r }
# Convert trans_time column to character
transactions_mutate_3 <- transactions %>%
  mutate(trans_time = as.character(trans_time),
         trans_hour = as.numeric(str_sub(trans_time, end = -3)))

glimpse(transactions_mutate_3)
```

### arrange

You can order tables by certain columns. In this example, this would be helpful in order to see transactions made by the same household next to each other rather than being randomised.

```{r }
# Example 1
transactions_arrange_1 <- transactions %>%
  arrange(household_key)

glimpse(transactions_arrange_1)
```

You can also order by descending order

```{r }
# Example 2
transactions_arrange_2 <- transactions %>%
  arrange(desc(household_key))

glimpse(transactions_arrange_2)
```

You can also order by multiple columns. In this example, it could be useful in order to show transactions ordered by household, but also by the week so that you can compare their first transaction to their most recent transactions.

```{r }
# Example 3
transactions_arrange_3 <- transactions %>%
  arrange(household_key, week_no)

glimpse(transactions_arrange_3)
```

### joins

You may have several tables that can be joined together. In this example, we have been looking at transactional data and may have meta data about products in a separate table.

```{r }
# Read in product table
products <- dbGetQuery(con, "select * from dunnhumby.products")

glimpse(products)
```

This table contains a `product_id` column that also appears in the `transactions` table. There are different ways you can join e.g. left, right, full, inner... this won't be covered in this tutorial but is worth looking into if you have not used them before.

```{r }
# Example 1
transactions_join_1 <- transactions %>%
  left_join(products, by = "product_id")

glimpse(transactions_join_1)
```

If the columns have different names you can use the following syntax (e.g. if you change the `product_id` column in the product table with just `id`):

```{r }
# Example 2
products_2 <- products %>%
  rename(id = product_id)

transactions_join_2 <- transactions %>%
  left_join(products_2, by = c("product_id" = "id"))

glimpse(transactions_join_1)
```

You can also join tables on multiple columns. In this example it is not required, but could look something like this...

```{r }
# Example 3
## Add an extra column called id (this is completely unecessary, and only being done for this example)
products_3 <- products %>%
  mutate(id = product_id)

## Join the tables on multiple columns
transactions_join_3 <- transactions %>%
  left_join(products_3, by = c("product_id" = "id", "product_id" = "product_id"))

glimpse(transactions_join_3)
```

## Data aggregations using dplyr

The functions above help you to tidy up and transform your data & the next step is aggregating it.

* `group_by` allows you to aggregate data, split by a particular column
* `summarise` allows you to define the types of aggregations you'd like to perform

The functions below are commonly used for aggregations:
* `min` & `max` identifies the minimum and maximum value in the data
* `median` & `mean` aggregates the average across the data
* `sum` sums a numeric column
* `n_distinct` counts the distinct number of entries
* `n()` counts the number of rows in the group (better practise to use `n_distinct`)

The examples below provide some summary statistics about the whole data set:

```{r }
# Calculate aggregations across the whole data set
## Number of rows
## Number of unique households
## Total sales value
## Minimum sales value
## Maximum sales value

aggregations_1 <- transactions %>%
  summarise(n_rows = n(),
            n_households = n_distinct(household_key),
            total_sales_value = sum(sales_value),
            min_sales_value = min(sales_value),
            max_sales_value = max(sales_value))

glimpse(aggregations_1)
```

Here are a few grouping examples

* In a forecasting project, you may need to aggregate your data over time e.g. you might want to forecast the total sales value by week
* In a customer segmentation project, you may want to identify your most valuable customers (based on sales value)

```{r }
# Time series example
aggregations_2 <- transactions %>%
  group_by(week_no) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  arrange(week_no)

glimpse(aggregations_2)
```

```{r }
# Customer grouping example to find most valuable customers
aggregations_3 <- transactions %>%
  group_by(household_key) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  arrange(desc(total_sales_value))

glimpse(aggregations_3)
```

You can also do useful things like filtering the first row of each group. For example, you might want to get the top product department bought by each customer.

```{r }
# Find each customers top product department
## Join on the products table
## Group by household and department to get the total sales in each department
## Order the departments in descending order in terms of sales value
## Filter the first row for each household to get their top category!
aggregations_4 <- transactions %>%
  left_join(products, by = "product_id") %>%
  group_by(household_key, department) %>%
  summarise(department_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  group_by(household_key) %>%
  arrange(desc(department_sales_value)) %>%
  filter(row_number() == 1) %>%
  ungroup() %>%
  arrange(household_key)

glimpse(aggregations_4)
```

## How can dbplyr help with all of this?

`dbplyr` is a very similar to `dplyr`, and intentionally has lots of the same functions! The main difference is that `dplyr` functions work on data that you have locally (i.e. data that is loaded into your environment) whereas `dbplyr` works with data stored in data bases instead.

`dbplyr` does this by converting `dplyr` functions into SQL, so that the queries can be run on a Redshift database. The benefit of this is that if your data is really large, you can perform aggregations over it without having to read it all into the Workspace (and risk crashing it!).

Even if you're a SQL pro, some things are just easier to write in `dbplyr` rather than writing the equivalent SQL code, so it's worth learning so you can switch between the two.

To get started with `dbplyr`, you have to connect to a data base and point the connection to tables you wish to analyse.

```{r }
# Connect to Redshift
con <- dbConnect(odbc::odbc(), "newstarter-prod", bigint = "integer")

# Point to the tables in Redshift you'd like to use
transactions <- con %>%
  tbl(in_schema("dunnhumby", "transactions"))

products <- con %>%
  tbl(in_schema("dunnhumby", "products"))
```

You can then perform all of the same functions on these tables as you would with `dplyr`. For example, you can join these tables together as follows:

```{r }
# Join tables
transactions_products <- transactions %>%
  left_join(products, by = "product_id")
```

All of these actions are being performed in the database, so at some point you need to tell R to actually read in the data you'd like to use, using the `collect` function.

The point of using `dbplyr` is to minimise the amount of data you're loading into R, so make sure that you collect when the data you're reading in is small.

For example, you could run `transactions %>% collect()` to read in all 2.5m rows of this data, but this will be slow and would also not be possible if the data was larger than this.

Here are a few useful commands you can run when getting to know your data - this should all seem familiar from the `dplyr` examples, but all have `collect` at the end to read the data from Redshift into R.

```{r }
# Read in sample of data
transactions_dbplyr_1 <- transactions %>%
  head(10) %>%
  collect()

glimpse(transactions_dbplyr_1)
```

```{r }
# Count the number of rows in the data
aggregations_dbplyr_1 <- transactions %>%
  summarise(n_rows = n()) %>%
  collect()

glimpse(aggregations_dbplyr_1)
```

The examples we went through earlier in `dplyr` can all be repeated to get the same result!

```{r }
# Time series example
aggregations_dbplyr_2 <- transactions %>%
  group_by(week_no) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  arrange(week_no) %>%
  collect()

glimpse(aggregations_dbplyr_2)
```

```{r }
# Customer grouping example to find most valuable customers
aggregations_dbplyr_3 <- transactions %>%
  group_by(household_key) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  arrange(desc(total_sales_value)) %>%
  collect()

glimpse(aggregations_dbplyr_3)
```

You can also do useful things like filtering the first row of each group. For example, you might want to get the top product department bought by each customer.

```{r }
# Find each customers top product department
## Join on the products table
## Group by household and department to get the total sales in each department
## Order the departments in descending order in terms of sales value
## Filter the first row for each household to get their top category!
aggregations_dbplyr_4 <- transactions %>%
  left_join(products, by = "product_id") %>%
  group_by(household_key, department) %>%
  summarise(department_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  group_by(household_key) %>%
  arrange(desc(department_sales_value)) %>%
  filter(row_number() == 1) %>%
  ungroup() %>%
  arrange(household_key) %>%
  collect()

glimpse(aggregations_dbplyr_4)
```

## Making charts using ggplot2

`ggplot2` is the most commonly used package to create charts in R. It is mainly based on the concept of "layers" - so you start by defining the data layer, then will add features to the plot using the `+` sign. That might sound weird, but it should make sense when you see a few examples!

The main types of charts to know about are as follows:

* `geom_line` creates line charts
* `geom_col` creates bar charts
* `geom_point` creates scatter charts

Let's use the time series example from the previous sections. We'll use `dbplyr` to aggregate the total sales in each week in Redshift & read this into R. Then we will use `ggplot2` to visualise this to understand if there are any interesting trends.

```{r }
# Connect to Redshift
con <- dbConnect(odbc::odbc(), "newstarter-prod", bigint = "integer")

# Point to the tables in Redshift you'd like to use
transactions <- con %>%
  tbl(in_schema("dunnhumby", "transactions"))

# Aggregate data using dbplyr
time_series <- transactions %>%
  group_by(week_no) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  arrange(week_no) %>%
  collect()

# Use ggplot2 to make a line chart
## Define the "data layer" by stating which data frame to use & the x, y axis values
## Add what kind of chart it should be e.g. geom_line for a line chart
ggplot(data = time_series, mapping = aes(x = week_no, y = total_sales_value)) +
  geom_line()
```

Now you have a basic line chart, you can tweak it by adding axis labels, changing the axes and adding colours! In this example, the numbers on the y-axis labels are not presented in the best way - these either need to be comma separated, or abbreviated to 'k'.

```{r }
# Chart formatting
## Add a colour for the line
## Change axis labels
## Add readable axis titles
## Change the font
## Remove the dark grey background by changing the theme of the "panel"
ggplot(data = time_series, mapping = aes(x = week_no, y = total_sales_value)) +
  geom_line(colour = "#66ff99") +
  scale_y_continuous(labels = function (x) paste0(x / 1000, 'k')) +
  labs(x = "Week", y = "Total sales value") +
  theme(text = element_text(family = "TeX Gyre Heros"),
        panel.background = element_blank(),
        panel.grid.major   = element_line(colour = "#f0f0f0"))
```

To save you formatting all of your charts from scratch each time, the Peak Data Science team have developed an internal package called `peaktheme` which has some useful functions.

```{r }
# Use theme_peak to format your charts with Peak fonts, correct backgrounds etc.
ggplot(data = time_series, mapping = aes(x = week_no, y = total_sales_value)) +
  geom_line() +
  scale_y_continuous(labels = function (x) paste0(x / 1000, 'k')) +
  labs(x = "Week", y = "Total sales value") +
  theme_peak()
```

You can also display data from different "groups" on the same chart. In this example, there are two brands of products, Private and National - so you may want to plot total sales for both of these over time.

These two functions in `peaktheme` are particularly useful!

* `scale_colour_peak` sets Peak colours when a `colour` argument is set (line charts, scatter charts)
* `scale_fill_peak` sets Peak colours when a `fill` argument is set (bar charts)

```{r }
# Connect to Redshift
con <- dbConnect(odbc::odbc(), "newstarter-prod", bigint = "integer")

# Point to the tables in Redshift you'd like to use
transactions <- con %>%
  tbl(in_schema("dunnhumby", "transactions"))

products <- con %>%
  tbl(in_schema("dunnhumby", "products"))

# Aggregate data
time_series_group <- transactions %>%
  left_join(products, by = "product_id") %>%
  group_by(week_no, brand) %>%
  summarise(total_sales_value = sum(sales_value)) %>%
  ungroup() %>%
  collect()

# Make a chart with different lines for each brand of product
## Note that for line charts you also need to include group = brand
ggplot(data = time_series_group, mapping = aes(x = week_no, y = total_sales_value,
                                               colour = brand, group = brand)) +
  geom_line() +
  scale_colour_peak() +
  scale_y_continuous(labels = function (x) paste0(x / 1000, 'k')) +
  labs(x = "Week", y = "Total sales value", colour = 'Brand') +
  theme_peak()
```

If you'd rather show the lines on separate, side by side charts, you can use `facet_wrap`.

```{r }
# Add facet_wrap if you'd rather have the lines show up on different charts
ggplot(data = time_series_group, mapping = aes(x = week_no, y = total_sales_value,
                                               colour = brand, group = brand)) +
  geom_line() +
  scale_colour_peak() +
  scale_y_continuous(labels = function (x) paste0(x / 1000, 'k')) +
  labs(x = "Week", y = "Total sales value", colour = 'Brand') +
  theme_peak() +
  facet_wrap('brand')
```

The `peaktheme` package contains a `barchart` and `linechart` function to make this easy - it'll set all of the Peak colours etc. for you!

```{r }
# Use peaktheme to create linechart
linechart(data_frame = time_series_group, x = 'week_no', y = 'total_sales_value', group = 'brand',
          x_label = 'Week', y_label = 'Total sales value', group_label = 'Brand')
```


