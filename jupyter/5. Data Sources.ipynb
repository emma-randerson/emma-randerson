{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Orion_ is Peak's go to framework for reading, writing & transferring data across various sources.\n",
    "\n",
    "In this tutorial we will go over some of the most foundamental operations we perform at Peak, such as writing data to/from Redshift/S3/AIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.sources import S3Source\n",
    "\n",
    "weather = S3Source(\n",
    "    bucket='kilimanjaro-prod-datalake', \n",
    "    key='newstarter/uploads/weather/1581525070376_Peak_weather.csv'\n",
    ").read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike S3, Redshift requires access to certain environment variables before any read/write operation can be executed. We pull those environment variables into our notebook using _load_env()_ function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Using Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.contrib.envs import load_env\n",
    "load_env()\n",
    "\n",
    "from orion.sources import RedshiftSource\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    select \n",
    "        *\n",
    "    from\n",
    "        stage.weather\n",
    "    \"\"\"\n",
    "\n",
    "weather = RedshiftSource(query=sql_query).read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Using File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.contrib.envs import load_env\n",
    "load_env()\n",
    "\n",
    "from orion.sources import RedshiftSource\n",
    "\n",
    "sql_file = \"resources/weather.sql\"\n",
    "\n",
    "weather =  RedshiftSource(query_file=sql_file).read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Data Out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.sources import S3Source\n",
    "\n",
    "S3Source(\n",
    "    bucket=\"kilimanjaro-prod-datalake\", \n",
    "    key=\"newstarter/datascience/weather.csv\"\n",
    ").write_csv(weather, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to write to Redshift you need special environment variable called _Redshift IAM Role_.\n",
    "\n",
    "To get this variable you will have to make a support ticket (https://peak-bi.atlassian.net/servicedesk/customer/portals).\n",
    "\n",
    "Once you receive it from DevOps team member, you will need to do the following:\n",
    "\n",
    "1. Click on the `+` icon in the top left corner.\n",
    "2. Click on Terminal.\n",
    "3. Type in `cd ~/` to make sure you are in the user root directory.\n",
    "4. Type in `nano .env`.\n",
    "5. At the bottom of the file enter: `export REDSHIFT_IAM_ROLE=<redshift-iam-role>`. <br>\n",
    "    5.1 Where `redshift-iam-role` is the _Redshift IAM Role_.\n",
    "6. Save changes `CTRL+X > Y > ENTER`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Existing Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create fake data sample to how a new data sample can be appending to an existing table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.contrib.envs import load_env\n",
    "load_env()\n",
    "\n",
    "from orion.sources import RedshiftSource\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "new_sample = pd.DataFrame([\n",
    "    {\n",
    "        'origin':np.random.choice(weather['origin']),\n",
    "        'year': datetime.now().year,\n",
    "        'month': datetime.now().month,\n",
    "        'day': datetime.now().day,\n",
    "        'hour': datetime.now().hour,\n",
    "        'temp': np.random.choice(weather['temp']),\n",
    "        'dewp': np.random.choice(weather['dewp']),\n",
    "        'humid': np.random.choice(weather['humid']),\n",
    "        'wind_dir': np.random.choice(weather['wind_dir']),\n",
    "        'wind_speed': np.random.choice(weather['wind_speed']),\n",
    "        'wind_gust': np.random.choice(weather['wind_gust']),\n",
    "        'precip': np.random.choice(weather['precip']),\n",
    "        'pressure': np.random.choice(weather['pressure']),\n",
    "        'visib': np.random.choice(weather['visib']),\n",
    "        'time_hour': datetime.now().strftime(format=\"%Y-%m-%dT%H:%M:%S%Z\")\n",
    "    }\n",
    "], columns=weather.columns)\n",
    "\n",
    "RedshiftSource(schema='stage', table='weather').write_csv(new_sample, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 New Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Orion_ will automatically create a new table according to provided DF's schema if the `table` parameter does not match to the existing table.\n",
    "\n",
    "`overwrite=True` flag will truncate the table and copy the contents of DF into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.contrib.envs import load_env\n",
    "load_env()\n",
    "\n",
    "from orion.sources import RedshiftSource\n",
    "\n",
    "RedshiftSource(table=\"new_weather\", schema=\"stage\", overwrite=True).write_csv(weather, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 AIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to study Notebook **4. AIS API KEY** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from orion.contrib.envs import load_env\n",
    "load_env()\n",
    "\n",
    "from orion.sources import AISSource\n",
    "\n",
    "AISSource(\n",
    "    target=\"weather/weather_20200227.csv\", \n",
    "    key=os.environ['AIS_API_KEY'], \n",
    "    token=os.environ['AIS_API_KEY']\n",
    ").write_csv(weather, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify success of the upload by going to _AIS > Outcomes > Downloads_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
